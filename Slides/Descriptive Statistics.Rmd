---
title: "Descriptive Statistics"
author: "Aleksandr Fisher"
output: beamer_presentation
theme: focus
latex_engine: xelatex
highlight: zenburn
header-includes:
   - \usepackage{animate}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(size = "footnotesize")
```

## Plan

1. Overview of Last Week
2. Measurement 
3. Descriptive Statistics
4. Readings
5. Wrap up

## Measurement

* Social science is about developing and testing causal theories:
    + Does minimum wage change levels of employment?
    + Does outgroup contact influence views on immigration?
* Theories are made up of concepts:
    + Minimum wage, level of employment, outgroup contact, views on
immigration.
    + We took these for granted when talking about causality.
* Important to consider how we measure these concepts.
    + Some more straightforward: what is your age?
    + Others more complicated: what does it mean to “be liberal”?
    + Have to create an operational definition of a concept to make it into a
variable in our dataset.

## Example

- Concept: presidential approval.
- Conceptual definition:
    - Extent to which US adults support the actions and policies of the current US
president.
    - Operational definition:
- “On a scale from 1 to 5, where 1 is least supportive and 5 is more supportive,
how much would you say you support the job that Donald Trump is doing as
president?”

## Measurement Error

- **Measurement error**: chance variation in our measurements.
    - individual measurement = exact value + chance error
    - chance errors tend to cancel out when we take averages.
- No matter how careful we are, a measurement could have always come out
differently.
    - Panel study of 19,000 respondents: 20 reported being a citizen in 2010 and
then a non-citizen in 2012.
  - Data entry errors.
- **Bias**: systematic errors for all units in the same direction.
    - individual measurement = exact value + bias + chance error.
    - “Did you vote?” ~ overreporting

## Definitions

- A **variable** is a series of measurements about some concept.
- **Descriptive statistics** are numerical summaries of those measurements.
  - If we smart enough, we wouldn’t need them: just look at the list of numbers
and completely understand.
- Two salient features of a variable that we want to know:
  - **Central tendency**: where is the middle/typical/average value.
  - **Spread** around the center: are all the data close to the center or spread out?

## Center of the Data

- "Center" of the data: Typical/average value
- **Mean**: sum of the values divided by the number od observations
- **Median**: the "middle" of a sorted list of numbers.
- Median more robust to outliers
  - Example 1: data = {0, 1, 2, 3, 5}, mean = 2.2, median = 2
  - Example 2: data = {0, 1, 2, 3, 100}, mean = 21.2, median = 2
  
## Spread of the data

- Are the data close to the center?
- **Range**: [min(x), max(x]
- **Quantile** (quartile, quintile, percentile, etc):
    - 25th percentile = lower quartile (25% of the data below this value)
    - 50th percentile = median (50% of the data below this value)
    - 75th percentile = upper quartile (75% of the data below this value)
- **Interquartile range (IQR)**: a measure of variability
    - How spread out is the middle half of the data?
    - Is most of the data really close to the median or are the values spread out?
- One definition of outliers: over 1.5 × IQR above the upper quartile or below
lower quartile.

## Standard deviation

- **Standard deviation**: On average, how far away are data points from the
mean?
$$\sqrt\frac{\sum((x - \bar{x})^{2})}{n-1}$$
- Steps:

    1. Subtract each data point by the mean
    2. Square each resulting difference
    3. Take the sum of these values
    4. Divide by n-1
    5. Take the square root

## Variance

- **Variance** = standard deviation (squared)

$$\frac{\sum((x - \bar{x})^{2})}{n-1}$$

## How large is large?

- Need a way to put any variable on common units.
-  **z-score**: $$\frac{x - \bar{x}}{\sigma}$$

- Interpretation:
    - Positive values above the mean, negative values below the mean
    - Units now on the scale of standard deviations away from the mean
    - Intuition: data more than 3 SDs away from mean are rare.

## Shape

- _Skewness_
  - Positive/right skew
  - Symmetric
  - Negative/left skew
  
- _Kurtosis_: peakedness of a distribution

## Skewness

```{r, echo=FALSE, fig.cap="Skewness", out.width = '90%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/skew.png")
```

## Kurtosis

```{r, echo=FALSE, fig.cap="Kurtosis", out.width = '90%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/kurt.jpg")
```

## Correlation

- How do variables move together on average?
- If I know one variable is big, does that tell me anything about how big the
other variable is?
    - Positive correlation: when X is big, Y is also big
    - Negative correlation: when X is big, Y is small
    - High correlation: data cluster tightly around a line.

## Whats is a strong correlation?

- In social sciences, usually we consider

    - lower than 0.3 = weak
    - between 0.3 and 0.5 = moderate
    - higher than 0.5 = strong
    - (but don't quote me on that!)

## Relationship

- **Covariance**: provides a measure of the strength of the correlation between two or more sets of random variates.

$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$

- **Correlation** is the degree to which two or more quantities are linearly associated

$$r = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}$$




## Correlation


```{r, echo=FALSE, fig.cap="Correlation by hand", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/corrbyhand.png")
```

## Correlation

- Correlation is **Positive** when the values **increase** together
- Correlation is **Negative** when one value **decreases** as the other increases

- Correlation can have a value:

  - 1 is a perfect **positive** correlation
  - 0 is no correlation (the values don't seem linked at all)
  - -1 is a perfect **negative** correlation

## Correlation

```{r, echo=FALSE, fig.cap="Correlation", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/correlation.png")
```

## Properties of correlation coefficient

- Correlation measures **linear** association.
- Interpretation:
    - Correlation is between -1 and 1
    - Correlation of 0 means no linear association.
    - Positive correlations ~ positive associations.
    - Negative correlations ~ negative associations.
    - Closer to -1 or 1 means stronger association.
- Order doesn’t matter: cor(x,y) = cor(y,x)
- Not affected by changes of scale:
    - Celsius vs. Fahreneheit; dollars vs. pesos; cm vs. in.


## Correlation is not good at curves

```{r, echo=FALSE, fig.cap="Anscombe's Quartet", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/ans.png")
```

## Correlation is not Causation

- What it really means is that a correlation does not prove one thing causes the other:

  - One thing might cause the other
  - The other might cause the first to happen
  - They may be linked by a different thing
  - Or it could be random chance!

## Correlation is not causation

- Any correlation is potentially causal
    - X might cause Y
    - Y might cause X
    - X and Y might be caused by Z
    - X and Y might cause Z
    - There may be no causal relationship

## Spurious Correlations

```{r, echo=FALSE, fig.cap="Correlation is NOT Causation", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/cage.png")
```

## Naive Causal Inference

- Correlations are not necessarily causal

- Our mind thinks they are because humans are not very good at the kind of
causal inference problems that social scientists care about

- Instead, we’re good at understanding **physical causality**

## Physical Causality

- Action and reaction
- Example:
    - Picture a ball resting on top of a hill
    - What happens if I push the ball?
- Features:
    - Observable
    - Single-case
    - Deterministic
    - Monocausal
    
## Pre-Post Change Heuristic

- Our intuition about causation relies too heavily on simple comparisons of
pre-post change in outcomes before and after something happens
- Why can this be wrong?

## Flaws in causal inference from pre-post comparisons

- Maturation or trends
- Regression to the mean
- Selection
- Simultaneous historical changes
- Instrumentation changes
- Monitoring changes behaviour

## Maturation or trends

- Is a shift in an outcome before and after a policy change the impact of the policy or a small part of a longer time trend?
- Example:

## Regression to the mean

- Is a shift in an outcome before and after a policy change the impact of the policy or a function of statistical variation?
- Example:

## Selection

- Is a shift in an outcome before and after a policy the impact of the policy or the result of the policy being implemented when outcomes are extreme?


## Simultaneous changes

- Is the shift in an outcome before and after a policy the impact of the policy or the result of a simultaneous historical shift?

## Instrumentation changes

- Is the shift in an outcome before and after a policy the impact of the policy or a change in how the outcome is measured?

## Monitoring changes behaviour

- Is the shift in an outcome before and after a policy the impact of the policy or a change in response to measuring the outcome per se?

## Examples

- Age and conservatism
- GDP and democracy
- Personality traits and political ideologies
- Healthcare spending and happiness

## In R...

- `mean()`
- `median()`, `min()`, `max()`, `quantile()`
- `var()`
- `sd()`
- `cov()`
- `cor()`

## Studying Feelings Toward Democracy

- 2018 Pew Study
- 27 countries, ~ 30,000 respondents
- **Question**: How satisfied are you with the way democracy is working in our country – very satisfied, somewhat satisfied, not too satisfied, or not at all satisfied? 
    1. very satisfied
    2. somewhat satisfied
    3. not too satisfied
    4. not at all satisfied
    8. don't know
    9. refused

## Data

- Load the data:

\tiny
```{r, warning=FALSE, message=FALSE}
library(haven) # package to read the data
library(dplyr) # package for data manipulation
library(tidyverse) # package for 'tidy' data
pew2018 <- read_sav("Pew 2018.sav",
                  user_na=TRUE) %>%
                  as_factor()
pew2018 = pew2018 %>% select(COUNTRY, satisfied_democracy, age, sex, d_ptyid_us)
pew2018$partyid2 <- fct_collapse(pew2018$d_ptyid_us,
                         DK = c("No preference (DO NOT READ)", 
                                     "Other party (DO NOT READ)",
                                     "Don’t know (DO NOT READ)",
                                     "Refused (DO NOT READ)"),
                         Rep = "Republican",
                         Ind = "Independent",
                         Dem = "Democrat")
```
\normalsize

## Glimpsing at data

- `dim()`: Retrieve the dimension

- `names()`: Get the names

- `str()`: Display compactly the internal structure
  
- `glimpse()`: is the dplyr-version of str() showing values of each variable the whole sceen width, but does not display the number of levels and names of factor variables. But this feature of str() cannot be displayed completly with either many or long levels names.

- `View()`: With RStudio you can see and inspect the data set comfortably. The View() function invokes a spreadsheet-style data viewer.

## Glimpsing at data

\tiny
```{r, warning=FALSE, message=FALSE}
dim(pew2018) # Dimensions
names(pew2018) # Column names
glimpse(pew2018) # Structure of data
```
\normalsize

## Contingency table

- The `_table()_` function shows us how many respondents are in each
category of a categorical variable:

\tiny
```{r, warning=FALSE, message=FALSE}
table(pew2018$satisfied_democracy)
```
\normalsize

- We can use prop.table() to show what proportions of the data each
response represents:

\tiny
```{r, warning=FALSE, message=FALSE}
prop.table(table(pew2018$satisfied_democracy))
```
\normalsize

## Barplot
- The `barplot()` function can help us visualize a contingency table:
 
\tiny
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
barplot(prop.table(table(pew2018$satisfied_democracy)), 
xlab = "Self-reported satisfaction with democracy", 
ylab = "Proportion of respondents")
```
\normalsize

- Arguments:
    - First is the height each bar should take (we’re using proportions in this case)
    - names are the labels for the each category
    - xlab, ylab are axis labels
  
## Barplot


\tiny
```{r, message=FALSE, warning=FALSE, echo=FALSE}
barplot(prop.table(table(pew2018$satisfied_democracy)), 
        names=c("Very satisfied",
                "Somewhat satisfied",
                "Somewhat dissatisfied",
                "Very dissatisfied",
                "DK",
                "Refused"), 
xlab = "Self-reported satisfaction with democracy", 
ylab = "Proportion of respondents", cex.names=0.5)

```

## Bar plot (Using ggplot)

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(scales)
ggplot(pew2018)+
  aes(x=satisfied_democracy)+
  geom_bar(aes(y=..prop.., group = 1)) +
  scale_y_continuous(labels=scales::percent_format())+
  labs(x ="Self-reported satisfaction with democracy", y = " Percent")+
  theme(plot.title = element_text(),
    axis.title.x = element_text(),
    axis.title.y = element_text(),
    axis.text.y = element_text(),
    axis.text.x = element_text(size=10)) +
  scale_x_discrete(labels=c("Very satisfied",
                "Somewhat satisfied",
                "Somewhat dissatisfied",
                "Very dissatisfied",
                "DK",
                "Refused"))
```
\normalsize

## Histogram

- Visualize density of continuous/numeric variable.
- How to create a histogram by hand:
    1. create bins along the variable of interest
    2. count number of observations in each bin
    3. **density** = bin height

- In R, we use `hist()` with `freq = FALSE`:

\small
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
hist(as.numeric(pew2018$age), freq = FALSE, ylim = c(0, 0.04),
xlab = "Age", main = "Distribution of Respondent's Age")
```
\normalsize

- Other arguments:
    - `ylim` sets the range of the y-axis to show (if you don’t set it, uses the range
of the data).
    - `main` sets the title for the figure.

## Histogram

```{r, message=FALSE, warning=FALSE, echo=FALSE}
hist(as.numeric(pew2018$age), freq = FALSE, 
     ylim = c(0, 0.04),
     xlab = "Age", 
     main = "Distribution of Respondent's Age")
abline(v=median(as.numeric(pew2018$age)), col = "dodgerblue", lwd = 2)
```

## What is Density

- The areas of the blocks = proportion of observations in those blocks.
- area of the blocks sum to 1 (100%)
- Can lead to confusion: height of block can go above 1!

## Boxplot

- A boxplot can characterize the distribution of continuous variables
- Use `boxplot()`:

\small
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
boxplot(as.numeric(pew2018$age), 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```
\normalsize

- “Box” represents range between lower and upper quartile.
- “Whiskers” represents either:
    - 1.5 × IQR or max/min of the data, whichever is tinyer.
    - Points beyond whiskers are outliers


    
## Boxplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}
boxplot(as.numeric(pew2018$age), 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```

## Comparing Distributions with the boxplot

- Useful for comparing a variable across groups:

\small
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
boxplot(as.numeric(pew2018$age) ~ pew2018$sex, 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```
\normalsize

- First argument is called a formula, `y ~ x`:
    - `y` is the continuous variable whose distribution we want to explore.
    - `x` is the grouping variable.
    - When using a formula, we need to add a `data` argument
    
## Comparing Distributions with the boxplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}
boxplot(as.numeric(pew2018$age) ~ pew2018$partyid2, 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```

## Satisfaction with democracy by party
```{r, message=FALSE, warning=FALSE, echo=FALSE}
pew = pew2018 %>% filter(partyid2=="Dem" | partyid2=="Rep" | partyid2=="Ind")
library(ggplot2)
library(scales)
ggplot(pew)+
  aes(x=satisfied_democracy)+
  geom_bar(aes(y=..prop.., group = 1)) +
  scale_y_continuous(labels=scales::percent_format())+
  labs(x ="Self-reported satisfaction with democracy", y = " Percent")+
  theme(plot.title = element_text(),
    axis.title.x = element_text(),
    axis.title.y = element_text(),
    axis.text.y = element_text(),
    axis.text.x = element_text(size=10)) +
  scale_x_discrete(labels=c("Very satisfied",
                "Somewhat satisfied",
                "Somewhat dissatisfied",
                "Very dissatisfied",
                "DK",
                "Refused"))+
  coord_flip() +
  facet_wrap(~partyid2)
```

## Satisfaction with democracy by party

- Why are Republicans more satified with democracy?


## Correlations in R

- Use the `cor()` function
- Missing values: set the use = ”pairwise” ~ available case analysis

\tiny
```{r, message=FALSE, warning=FALSE}

# Read Happiness Data
happ2019 = read.csv("C:/Users/afisher/Documents/R Code/Resources/Data/Happiness/2019.csv")

# Structure of dataset
str(happ2019)

# Correlation
cor(happ2019$Score, happ2019$GDP.per.capita)
```
\normalsize

## QQ-plot example

- **Quantile-quantile plot (qq-plot)**: Plot the **quantiles** of each distribution
against each other.
- Example points:
    - (min of X, min of Y)
    - (median of X, median of Y)
    - (25th percentile of X, 25th percentile of Y)
- 45 degree line indicates quality of the two distributions

## QQ-plot example

\tiny
```{r, message=FALSE, warning=FALSE}

qqplot(happ2019$Score, happ2019$GDP.per.capita, xlab = 'Score', ylab="GDP per capita")
       
```
\normalsize

## Scatterplot

\footnotesize
```{r, message=FALSE, warning=FALSE, fig.keep='none'}

## Base R
plot(happ2019$Score, happ2019$GDP.per.capita, xlab = 'Score', ylab="GDP per capita")

## ggplot
ggplot(happ2019) +
  aes(x=Score, y=GDP.per.capita)+
  geom_point()+
  theme_classic()
```
\normalsize
## Base R - Scatterplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}

## Base R
plot(happ2019$Score, happ2019$GDP.per.capita, xlab = 'Score', ylab="GDP per capita")
```

## ggplot - Scatterplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}

## ggplot
ggplot(happ2019) +
  aes(x=Score, y=GDP.per.capita)+
  geom_point()
```



# Tables and Missing Data in R

## Tables in R

\footnotesize
```{r}

# Read File
afghan <- read.csv("C:/Users/afisher/Documents/R Code/qss/MEASUREMENT/afghan.csv")

# Column names
names(afghan)

# Tables
table(ISAF = afghan$violent.exp.ISAF,
      Taliban = afghan$violent.exp.taliban)
```
\normalsize

## Table in R: prop.table()

We can also use prop.table() to see the proportion of cases in each cell

We have to include table() within parentheses too:

\footnotesize
```{r}
prop.table(table(ISAF = afghan$violent.exp.ISAF,
                 Taliban = afghan$violent.exp.taliban))
```
\normalsize

## Round Function

- Since we're already using nested functions, we can also use round() to round the values in each cell

- Notice the `, 2` in the code below. It indicates that we will round the numbers up to two significant digits

\footnotesize
```{r}
round(prop.table(table(ISAF = afghan$violent.exp.ISAF,
                       Taliban = afghan$violent.exp.taliban)), 2)
```
\normalsize

## Missing Data

- Not all individuals answer to surveys

- Two types of non-response:

    - Individual non-response
    - Item non-response

- Both tend to bias the results

- So it is very important that we know where (and think about why) we see gaps in our data

## Missing Data in R

- R has a special code for missing data, `NA`

- Since `NA` is only used for missing observations, we can count their numbers with `is.na()`

\footnotesize
```{r}
head(afghan$income, 10)

# number of missings
sum(is.na(afghan$income)) 

# proportion of missings
round(mean(is.na(afghan$income)), 2) 
```
\normalsize

## Missing Data

- Some R function don't work if there's missing data

- We add `na.rm = TRUE` to the code

\footnotesize
```{r}
# Victims of Taliban violence
sum(is.na(afghan$violent.exp.taliban))

# Mean violance by taliban
mean(afghan$violent.exp.taliban)

# Mean violance by taliban
round(mean(is.na(afghan$income)), 2) 
```
\normalsize

- Why do you think we have missing data here?

## Missing Data in R

- You can also visualise the number of missing observations with `summary()`

```{r}
summary(afghan$violent.exp.taliban)
sum(is.na(afghan$violent.exp.taliban))
```

# Data Visualization

## Bar Plots

- Bar plots are used to visualise **factor/character variables**

- Proportion of observations in each category as the height of each bar

- Options:

    - `main = "Title"`
    - `xlab = "X label"`
    - `ylab = "Y label"`
    - `xlim = c(number, number)` limits for the x variable
    - `ylim = c(number, number)` limits for the y variable
    - `names.arg = c("Bars labels")` - in the same order of the variable
    - `horiz = TRUE` for horizontal plots
    - `cols` = "colour name" bar colour (see: )

- You can use `barplot()` with `prop.table()` instead of pie charts

## Barplots


```{r, fig.keep='none', results='asis'}
employed.ptable <- prop.table(table(afghan$employed))
employed.ptable

barplot(employed.ptable,
        names.arg = c("Unemployed", "Employed"), 
        main = "Proportion of Employed Afghanis",
        xlab = "Employment",
        ylab = "Proportion",
        ylim = c(0, 0.6))
```


## Barplots

```{r, echo=FALSE}
barplot(employed.ptable,
        names.arg = c("Unemployed", "Employed"), 
        main = "Proportion of Employed Afghanis",
        xlab = "Employment",
        ylab = "Proportion",
        ylim = c(0, 0.6))
```

## Horizontal Bar plot

\footnotesize
```{r, fig.keep='none', results='asis'}
barplot(employed.ptable,
        names.arg = c("Unemployed", "Employed"), # 0 and 1, respectively
        main = "Proportion of Employed Afghanis",
        ylab = "Employment", # change the axes
        xlab = "Proportion", 
        xlim = c(0, 0.7), # now it's xlim 
        horiz = TRUE,     # because the plot is horizontal
        col = "brown")
```
\normalsize

## Horizontal Bar plot

```{r, echo=FALSE}
barplot(employed.ptable,
        names.arg = c("Unemployed", "Employed"), # 0 and 1, respectively
        main = "Proportion of Employed Afghanis",
        ylab = "Employment", # change the axes
        xlab = "Proportion", 
        xlim = c(0, 0.7), # now it's xlim 
        horiz = TRUE,     # because the plot is horizontal
        col = "brown")
```


## Histogram

- We use histograms to display the distribution of a numeric variable

- Numeric variables are binned into groups

    - Histograms shows the density of each bin

    - Important: Height is share of observations in bin divided by bin size

- We care less about the density of each bin than about the distribution of the variable as a whole

- Area of each bar is the share of observations that fall into that bin

- Area of all bins sum to one

## Histogram

- Many options are similar to those of `barplot()`: `main`, `xlab`, `ylim`, `col`

- We can also add `freq = FALSE` to show the density of each histograms

- `breaks` = changes the size of the bins

- Densities are useful to compare different distributions

- _Densities are not percentages_: "percentage per horizontal unit"

## Histogram in R

```{r, fig.keep='none'}
hist(afghan$age,
     main = "Histogram - Age",
     xlab = "Age",
     xlim = c(0, 0.04),
     freq = FALSE,
     col = "darkorange2")
```

## Histogram in R

```{r, echo=FALSE}
hist(afghan$age,
     main = "Histogram - Age",
     xlab = "Age",
     freq = FALSE,
     col = "darkorange2")
```

## Histogram in R

- We can also add text and fitted lines to all R plots

- Use `text()` and `abline()` after `hist()`

```{r, fig.keep='none'}
hist(afghan$age,
     main = "Histogram - Age",
     xlab = "Age",
     xlim = c(0, 0.04),
     freq = FALSE,
     col = "darkorange2")
## add a text label at (x, y) = (35, 0.35)
text(x = 35, y = 0.035, "median")
## add a vertical line representing median
abline(v = median(afghan$age))
```

## Histogram in R

```{r, echo=FALSE}
hist(afghan$age,
     main = "Histogram - Age",
     xlab = "Age",
     freq = FALSE,
     col = "darkorange2")
## add a text label at (x, y) = (35, 0.35)
text(x = 35, y = 0.035, "median")
## add a vertical line representing median
abline(v = median(afghan$age))
```

## Box Plots

- Like histograms, box plots also display the distribution of a numeric variable

- Box plots show the median, quartiles, and IQR

- Useful to compare different distributions side-by-side

- It is also useful to identify outliers, that is, data points that are above 1.5 times the **interquartile range (IQR)**

- `boxplot()`

## Box Plots

- `boxplot()` also has a series of optional arguments:

    - `main`, `ylab`, `ylim`, `col`
    - `formula = y ~ group`, `y` is numeric variable and `group` is a factor
    
## Quantiles

\footnotesize
```{r, fig.keep='none'}
median(afghan$age)
quantile(afghan$age, probs = c(0, .25, .5, .75, 1))
boxplot(afghan$age, main = "Distribution of age", ylab = "Age", ylim = c(10, 80))
```
\normalsize

## Box Plots

```{r, echo=FALSE}
boxplot(afghan$age, main = "Distribution of age", ylab = "Age", ylim = c(10, 80))
```


## Box Plot

- But box plots provide an easy way to compare multiple observations at a time

- Similar to `tapply()`

\footnotesize
```{r, fig.keep='none'}

tapply(afghan$age, afghan$province, median, na.rm = TRUE)

boxplot(afghan$age ~ afghan$province,
        main = "Age Distribution by Province", ylab = "Age", col = "grey")

```
\normalsize

## Boxplot by group
```{r, echo=FALSE}
boxplot(afghan$age ~ afghan$province,
        main = "Age Distribution by Province", ylab = "Age", col = "grey")

```

# Survey Sampling

## Survey Sampling

```{r, echo=FALSE, fig.cap="", out.width = '90%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/polls.png")
```

- Most polls only interview several hundred voters
- Goal: infer what 200 million voters are thinking

## The 1936 Literary Digest Poll

- Mail questionnaire to 10 million people

- Final sample size: over 2.3 million returned

- Addresses came from phone books and club memberships

- The young Gallup used 50,000 respondents

- Predicted FDR would get 43% of the vote... actually recieved 62%

## WTF went wrong?

- Biased sample:

    - slanted toward middle- and upper-class voters, and by default to exclude lower-income voters
    - people who respond to surveys are different from people who don't
    
- Two morals of the story:

    - A badly chosen big sample is muc worse than a well-chosen small sample
    - Watch out for selection bias and nonresponse bias.
    
## Quota Sampling vs. Random Sampling

- **Quota sampling:** Sample certain groups until quota is filled

    - Problem: Unobservables can bias the results

- **Random sampling:** Random draws without replacement from the population

    - Everybody has the same chance of being in the sample

    - Problem: none, sample is unbiased ( in theory...)!
    
## Random Sampling

- Not every single sample will match all characteristics of the population exactly

- But as the number of samples gets larger (say 1000 samples of 1000 respondents), on average the samples would be representative

- Polls are associated with uncertainty: plus or minus a number

- But getting a random sample is hard

## Why is this hard

- Problems of telephone survey

    - Random digit dialing from phone book
    - Wealthy individuals have higher chances of being called
    - Caller ID screening (unit non-response)
    
- Problems of internet survey

    - Non-probability sampling
    - Cheap but non-representative
    - Young, urban, rich groups are overrepresented
    - Requires statistical corrections (usually weights)
    
## Social Desirability Bias

- Respondents sometimes do not state their true preferences

- Examples: support for drug use, abortion, etc

- Under- or overestimation of true proportion

## List Experiments

- List experiments can minimise the problem

- Grant anonymity to respondents

- Control group sees a list of statements

- Treatment group sees the same list plus a sensitive item

- Assuming that respondents don't lie and that both groups would answer the same number of non-sensitive items, we can infer their true preferences

## List Experiments

- https://statmodeling.stat.columbia.edu/2014/04/23/thinking-list-experiment-heres-list-reasons-think/




