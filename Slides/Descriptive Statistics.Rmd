---
title: "Descriptive Statistics"
author: "Aleksandr Fisher"
output: beamer_presentation
theme: metropolis
latex_engine: xelatex
highlight: zenburn

---


## Plan

1. Overview of Last Week
2. Measurement 
3. Descriptive Statistics
4. Readings
5. Wrap up

## Measurement

* Social science is about developing and testing causal theories:
    + Does minimum wage change levels of employment?
    + Does outgroup contact influence views on immigration?
* Theories are made up of concepts:
    + Minimum wage, level of employment, outgroup contact, views on
immigration.
    + We took these for granted when talking about causality.
* Important to consider how we measure these concepts.
    + Some more straightforward: what is your age?
    + Others more complicated: what does it mean to “be liberal”?
    + Have to create an operational definition of a concept to make it into a
variable in our dataset.

## Example

- Concept: presidential approval.
- Conceptual definition:
    - Extent to which US adults support the actions and policies of the current US
president.
    - Operational definition:
- “On a scale from 1 to 5, where 1 is least supportive and 5 is more supportive,
how much would you say you support the job that Donald Trump is doing as
president?”

## Measurement Error

- **Measurement error**: chance variation in our measurements.
    - individual measurement = exact value + chance error
    - chance errors tend to cancel out when we take averages.
- No matter how careful we are, a measurement could have always come out
differently.
    - Panel study of 19,000 respondents: 20 reported being a citizen in 2010 and
then a non-citizen in 2012.
  - Data entry errors.
- **Bias**: systematic errors for all units in the same direction.
    - individual measurement = exact value + bias + chance error.
    - “Did you vote?” ~ overreporting

## Definitions

- A **variable** is a series of measurements about some concept.
- **Descriptive statistics** are numerical summaries of those measurements.
  - If we smart enough, we wouldn’t need them: just look at the list of numbers
and completely understand.
- Two salient features of a variable that we want to know:
  - **Central tendency**: where is the middle/typical/average value.
  - **Spread** around the center: are all the data close to the center or spread out?

## Center of the Data

- "Center" of the data: Typical/average value
- **Mean**: sum of the values divided by the number od observations
- **Median**: the "middle" of a sorted list of numbers.
- Median more robust to outliers
  - Example 1: data = {0, 1, 2, 3, 5}, mean = 2.2, median = 2
  - Example 2: data = {0, 1, 2, 3, 100}, mean = 21.2, median = 2
  
## Spread of the data

- Are the data close to the center?
- **Range**: [min(x), max(x]
- **Quantile** (quartile, quintile, percentile, etc):
    - 25th percentile = lower quartile (25% of the data below this value)
    - 50th percentile = median (50% of the data below this value)
    - 75th percentile = upper quartile (75% of the data below this value)
- **Interquartile range (IQR)**: a measure of variability
    - How spread out is the middle half of the data?
    - Is most of the data really close to the median or are the values spread out?
- One definition of outliers: over 1.5 × IQR above the upper quartile or below
lower quartile.

## Standard deviation

- **Standard deviation**: On average, how far away are data points from the
mean?
$$\sqrt\frac{\sum((x - \bar{x})^{2})}{n-1}$$
- Steps:

    1. Subtract each data point by the mean
    2. Square each resulting difference
    3. Take the sum of these values
    4. Divide by n-1
    5. Take the square root

## Variance

- **Variance** = standard deviation (squared)

$$\frac{\sum((x - \bar{x})^{2})}{n-1}$$

## How large is large?

- Need a way to put any variable on common units.
-  **z-score**: $$\frac{x - \bar{x}}{\sigma}$$

- Interpretation:
    - Positive values above the mean, negative values below the mean
    - Units now on the scale of standard deviations away from the mean
    - Intuition: data more than 3 SDs away from mean are rare.

## Shape

- _Skewness_
  - Positive/right skew
  - Symmetric
  - Negative/left skew
  
- _Kurtosis_: peakedness of a distribution

## Skewness

```{r, echo=FALSE, fig.cap="Skewness", out.width = '90%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/skew.png")
```

## Kurtosis

```{r, echo=FALSE, fig.cap="Kurtosis", out.width = '90%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/kurt.jpg")
```

## Correlation

- How do variables move together on average?
- If I know one variable is big, does that tell me anything about how big the
other variable is?
    - Positive correlation: when X is big, Y is also big
    - Negative correlation: when X is big, Y is small
    - High correlation: data cluster tightly around a line.



## Relationship

- **Covariance**: provides a measure of the strength of the correlation between two or more sets of random variates.

$$cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$

- **Correlation** is the degree to which two or more quantities are linearly associated

$$r = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}$$




## Correlation


```{r, echo=FALSE, fig.cap="Correlation by hand", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/corrbyhand.png")
```

## Correlation

- Correlation is **Positive** when the values **increase** together
- Correlation is **Negative** when one value **decreases** as the other increases

- Correlation can have a value:

  - 1 is a perfect **positive** correlation
  - 0 is no correlation (the values don't seem linked at all)
  - -1 is a perfect **negative** correlation

## Correlation

```{r, echo=FALSE, fig.cap="Correlation", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/correlation.png")
```

## Properties of correlation coefficient

- Correlation measures **linear** association.
- Interpretation:
    - Correlation is between -1 and 1
    - Correlation of 0 means no linear association.
    - Positive correlations ~ positive associations.
    - Negative correlations ~ negative associations.
    - Closer to -1 or 1 means stronger association.
- Order doesn’t matter: cor(x,y) = cor(y,x)
- Not affected by changes of scale:
    - Celsius vs. Fahreneheit; dollars vs. pesos; cm vs. in.


## Correlation is not good at curves

```{r, echo=FALSE, fig.cap="Anscombe's Quartet", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/ans.png")
```

## Correlation is not Causation

- What it really means is that a correlation does not prove one thing causes the other:

  - One thing might cause the other
  - The other might cause the first to happen
  - They may be linked by a different thing
  - Or it could be random chance!

## Correlation is not causation

- Any correlation is potentially causal
    - X might cause Y
    - Y might cause X
    - X and Y might be caused by Z
    - X and Y might cause Z
    - There may be no causal relationship

## Spurious Correlations

```{r, echo=FALSE, fig.cap="Correlation is NOT Causation", out.width = '80%'}
knitr::include_graphics("C:/Users/afisher/Documents/R Code/R-Code-for-Political-Science/Slides/Images For Slides/cage.png")
```

## Naive Causal Inference

- Correlations are not necessarily causal

- Our mind thinks they are because humans are not very good at the kind of
causal inference problems that social scientists care about

- Instead, we’re good at understanding **physical causality**

## Physical Causality

- Action and reaction
- Example:
    - Picture a ball resting on top of a hill
    - What happens if I push the ball?
- Features:
    - Observable
    - Single-case
    - Deterministic
    - Monocausal
    
## Pre-Post Change Heuristic

- Our intuition about causation relies too heavily on simple comparisons of
pre-post change in outcomes before and after something happens
- Why can this be wrong?

## Flaws in causal inference from pre-post comparisons

- Maturation or trends
- Regression to the mean
- Selection
- Simultaneous historical changes
- Instrumentation changes
- Monitoring changes behaviour

## Maturation or trends

- Is a shift in an outcome before and after a policy change the impact of the policy or a small part of a longer time trend?
- Example:

## Regression to the mean

- Is a shift in an outcome before and after a policy change the impact of the policy or a function of statistical variation?
- Example:

## Selection

- Is a shift in an outcome before and after a policy the impact of the policy or the result of the policy being implemented when outcomes are extreme?


## Simultaneous changes

- Is the shift in an outcome before and after a policy the impact of the policy or the result of a simultaneous historical shift?

## Instrumentation changes

- Is the shift in an outcome before and after a policy the impact of the policy or a change in how the outcome is measured?

## Monitoring changes behaviour

- Is the shift in an outcome before and after a policy the impact of the policy or a change in response to measuring the outcome per se?

## Examples

- Age and conservatism
- GDP and democracy
- Personality traits and political ideologies
- Healthcare spending and happiness

## In R...

- `mean()`
- `median()`, `min()`, `max()`, `quantile()`
- `var()`
- `sd()`
- `cov()`
- `cor()`

## Studying Feelings Toward Democracy

- 2018 Pew Study
- 27 countries, ~ 30,000 respondents
- **Question**: How satisfied are you with the way democracy is working in our country – very satisfied, somewhat satisfied, not too satisfied, or not at all satisfied? 
    1. very satisfied
    2. somewhat satisfied
    3. not too satisfied
    4. not at all satisfied
    8. don't know
    9. refused

## Data

- Load the data:

\tiny
```{r, warning=FALSE, message=FALSE}
library(haven) # package to read the data
library(dplyr) # package for data manipulation
library(tidyverse) # package for 'tidy' data
pew2018 <- read_sav("Pew 2018.sav",
                  user_na=TRUE) %>%
                  as_factor()
pew2018 = pew2018 %>% select(COUNTRY, satisfied_democracy, age, sex, d_ptyid_us)
pew2018$partyid2 <- fct_collapse(pew2018$d_ptyid_us,
                         DK = c("No preference (DO NOT READ)", 
                                     "Other party (DO NOT READ)",
                                     "Don’t know (DO NOT READ)",
                                     "Refused (DO NOT READ)"),
                         Rep = "Republican",
                         Ind = "Independent",
                         Dem = "Democrat")
```
\normalsize

## Glimpsing at data

- `dim()`: Retrieve the dimension

- `names()`: Get the names

- `str()`: Display compactly the internal structure
  
- `glimpse()`: is the dplyr-version of str() showing values of each variable the whole sceen width, but does not display the number of levels and names of factor variables. But this feature of str() cannot be displayed completly with either many or long levels names.

- `View()`: With RStudio you can see and inspect the data set comfortably. The View() function invokes a spreadsheet-style data viewer.

## Glimpsing at data

\tiny
```{r, warning=FALSE, message=FALSE}
dim(pew2018) # Dimensions
names(pew2018) # Column names
glimpse(pew2018) # Structure of data
```
\normalsize

## Contingency table

- The `_table()_` function shows us how many respondents are in each
category of a categorical variable:

\tiny
```{r, warning=FALSE, message=FALSE}
table(pew2018$satisfied_democracy)
```
\normalsize

- We can use prop.table() to show what proportions of the data each
response represents:

\tiny
```{r, warning=FALSE, message=FALSE}
prop.table(table(pew2018$satisfied_democracy))
```
\normalsize

## Barplot
- The `barplot()` function can help us visualize a contingency table:
 
\tiny
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
barplot(prop.table(table(pew2018$satisfied_democracy)), 
xlab = "Self-reported satisfaction with democracy", 
ylab = "Proportion of respondents")
```
\normalsize

- Arguments:
    - First is the height each bar should take (we’re using proportions in this case)
    - names are the labels for the each category
    - xlab, ylab are axis labels
  
## Barplot


\tiny
```{r, message=FALSE, warning=FALSE, echo=FALSE}
barplot(prop.table(table(pew2018$satisfied_democracy)), 
        names=c("Very satisfied",
                "Somewhat satisfied",
                "Somewhat dissatisfied",
                "Very dissatisfied",
                "DK",
                "Refused"), 
xlab = "Self-reported satisfaction with democracy", 
ylab = "Proportion of respondents", cex.names=0.5)

```

## Bar plot (Using ggplot)

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(scales)
ggplot(pew2018)+
  aes(x=satisfied_democracy)+
  geom_bar(aes(y=..prop.., group = 1)) +
  scale_y_continuous(labels=scales::percent_format())+
  labs(x ="Self-reported satisfaction with democracy", y = " Percent")+
  theme(plot.title = element_text(),
    axis.title.x = element_text(),
    axis.title.y = element_text(),
    axis.text.y = element_text(),
    axis.text.x = element_text(size=10)) +
  scale_x_discrete(labels=c("Very satisfied",
                "Somewhat satisfied",
                "Somewhat dissatisfied",
                "Very dissatisfied",
                "DK",
                "Refused"))
```
\normalsize

## Histogram

- Visualize density of continuous/numeric variable.
- How to create a histogram by hand:
    1. create bins along the variable of interest
    2. count number of observations in each bin
    3. **density** = bin height

- In R, we use `hist()` with `freq = FALSE`:

\small
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
hist(as.numeric(pew2018$age), freq = FALSE, ylim = c(0, 0.04),
xlab = "Age", main = "Distribution of Respondent's Age")
```
\normalsize

- Other arguments:
    - `ylim` sets the range of the y-axis to show (if you don’t set it, uses the range
of the data).
    - `main` sets the title for the figure.

## Histogram

```{r, message=FALSE, warning=FALSE, echo=FALSE}
hist(as.numeric(pew2018$age), freq = FALSE, 
     ylim = c(0, 0.04),
     xlab = "Age", 
     main = "Distribution of Respondent's Age")
abline(v=median(as.numeric(pew2018$age)), col = "dodgerblue", lwd = 2)
```

## What is Density

- The areas of the blocks = proportion of observations in those blocks.
- area of the blocks sum to 1 (100%)
- Can lead to confusion: height of block can go above 1!

## Boxplot

- A boxplot can characterize the distribution of continuous variables
- Use `boxplot()`:

\small
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
boxplot(as.numeric(pew2018$age), 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```
\normalsize

- “Box” represents range between lower and upper quartile.
- “Whiskers” represents either:
    - 1.5 × IQR or max/min of the data, whichever is tinyer.
    - Points beyond whiskers are outliers
    
## Boxplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}
boxplot(as.numeric(pew2018$age), 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```

## Comparing Distributions with the boxplot

- Useful for comparing a variable across groups:

\small
```{r,  message=FALSE, warning=FALSE, results='hide', fig.keep = 'none'}
boxplot(as.numeric(pew2018$age) ~ pew2018$sex, 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```
\normalsize

- First argument is called a formula, `y ~ x`:
    - `y` is the continuous variable whose distribution we want to explore.
    - `x` is the grouping variable.
    - When using a formula, we need to add a `data` argument
    
## Comparing Distributions with the boxplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}
boxplot(as.numeric(pew2018$age) ~ pew2018$partyid2, 
ylab = "Age", 
main = "Distribution of Respondent's Age")
```

## Satisfaction with democracy by party
```{r, message=FALSE, warning=FALSE, echo=FALSE}
pew = pew2018 %>% filter(partyid2=="Dem" | partyid2=="Rep" | partyid2=="Ind")
library(ggplot2)
library(scales)
ggplot(pew)+
  aes(x=satisfied_democracy)+
  geom_bar(aes(y=..prop.., group = 1)) +
  scale_y_continuous(labels=scales::percent_format())+
  labs(x ="Self-reported satisfaction with democracy", y = " Percent")+
  theme(plot.title = element_text(),
    axis.title.x = element_text(),
    axis.title.y = element_text(),
    axis.text.y = element_text(),
    axis.text.x = element_text(size=10)) +
  scale_x_discrete(labels=c("Very satisfied",
                "Somewhat satisfied",
                "Somewhat dissatisfied",
                "Very dissatisfied",
                "DK",
                "Refused"))+
  coord_flip() +
  facet_wrap(~partyid2)
```

## Satisfaction with democracy by party

- Why are Republicans more satified with democracy?


## Correlations in R

- Use the `cor()` function
- Missing values: set the use = ”pairwise” ~ available case analysis

\tiny
```{r, message=FALSE, warning=FALSE}

# Read Happiness Data
happ2019 = read.csv("C:/Users/afisher/Documents/R Code/Resources/Data/Happiness/2019.csv")

# Structure of dataset
str(happ2019)

# Correlation
cor(happ2019$Score, happ2019$GDP.per.capita)
```
\normalsize

## QQ-plot example

- **Quantile-quantile plot (qq-plot)**: Plot the **quantiles** of each distribution
against each other.
- Example points:
    - (min of X, min of Y)
    - (median of X, median of Y)
    - (25th percentile of X, 25th percentile of Y)
- 45 degree line indicates quality of the two distributions

## QQ-plot example

\tiny
```{r, message=FALSE, warning=FALSE}

qqplot(happ2019$Score, happ2019$GDP.per.capita, xlab = 'Score', ylab="GDP per capita")
       
```
\normalsize

## Scatterplot

\footnotesize
```{r, message=FALSE, warning=FALSE, fig.keep='none'}

## Base R
plot(happ2019$Score, happ2019$GDP.per.capita, xlab = 'Score', ylab="GDP per capita")

## ggplot
ggplot(happ2019) +
  aes(x=Score, y=GDP.per.capita)+
  geom_point()
```
\normalsize
## Base R - Scatterplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}

## Base R
plot(happ2019$Score, happ2019$GDP.per.capita, xlab = 'Score', ylab="GDP per capita")
```

## ggplot - Scatterplot

```{r, message=FALSE, warning=FALSE, echo=FALSE}

## ggplot
ggplot(happ2019) +
  aes(x=Score, y=GDP.per.capita)+
  geom_point()
```


